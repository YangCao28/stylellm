"""
é…ç½®åŠ è½½å™¨ - æ”¯æŒä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
"""

import yaml
import os
from pathlib import Path
from typing import Dict, Any, Optional
from config import Config, ModelConfig, TrainingConfig, DataConfig, EvalConfig


def load_yaml_config(config_file: str = "train_config.yaml") -> Dict[str, Any]:
    """
    ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
    
    Args:
        config_file: YAMLé…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    if not os.path.exists(config_file):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
    
    with open(config_file, 'r', encoding='utf-8') as f:
        yaml_config = yaml.safe_load(f)
    
    return yaml_config


def merge_config(default_config: Config, yaml_config: Dict[str, Any]) -> Config:
    """
    åˆå¹¶é»˜è®¤é…ç½®å’ŒYAMLé…ç½®
    
    Args:
        default_config: é»˜è®¤é…ç½®å¯¹è±¡
        yaml_config: YAMLé…ç½®å­—å…¸
        
    Returns:
        åˆå¹¶åçš„é…ç½®å¯¹è±¡
    """
    config = Config()
    
    # åˆå¹¶æ¨¡å‹é…ç½®
    if 'model' in yaml_config:
        model_dict = yaml_config['model']
        if 'model_name_or_path' in model_dict:
            config.model.model_name_or_path = model_dict['model_name_or_path']
        if 'use_lora' in model_dict:
            config.model.use_lora = model_dict['use_lora']
        if 'lora_r' in model_dict:
            config.model.lora_r = model_dict['lora_r']
        if 'lora_alpha' in model_dict:
            config.model.lora_alpha = model_dict['lora_alpha']
        if 'lora_dropout' in model_dict:
            config.model.lora_dropout = model_dict['lora_dropout']
        if 'lora_target_modules' in model_dict:
            config.model.lora_target_modules = model_dict['lora_target_modules']
    
    # åˆå¹¶è®­ç»ƒé…ç½®
    if 'training' in yaml_config:
        training_dict = yaml_config['training']
        for key, value in training_dict.items():
            if hasattr(config.training, key):
                setattr(config.training, key, value)
    
    # åˆå¹¶æ•°æ®é…ç½®
    if 'data' in yaml_config:
        data_dict = yaml_config['data']
        for key, value in data_dict.items():
            if key == 'span_length_range' and isinstance(value, list):
                setattr(config.data, key, tuple(value))
            elif hasattr(config.data, key):
                setattr(config.data, key, value)
    
    # åˆå¹¶è¯„ä¼°é…ç½®
    if 'evaluation' in yaml_config:
        eval_dict = yaml_config['evaluation']
        for key, value in eval_dict.items():
            if hasattr(config.eval, key):
                setattr(config.eval, key, value)
    
    return config


def load_config(config_file: Optional[str] = None, use_yaml: bool = True) -> Config:
    """
    åŠ è½½é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨YAMLæ–‡ä»¶ï¼‰
    
    Args:
        config_file: YAMLé…ç½®æ–‡ä»¶è·¯å¾„
        use_yaml: æ˜¯å¦ä½¿ç”¨YAMLé…ç½®
        
    Returns:
        é…ç½®å¯¹è±¡
    """
    # é»˜è®¤é…ç½®
    config = Config()
    
    if not use_yaml:
        return config
    
    # æŸ¥æ‰¾YAMLé…ç½®æ–‡ä»¶
    if config_file is None:
        # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾
        possible_files = [
            "train_config.yaml",
            "config.yaml",
            "train_config.yml",
            "config.yml",
        ]
        
        for file in possible_files:
            if os.path.exists(file):
                config_file = file
                break
    
    if config_file and os.path.exists(config_file):
        print(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
        yaml_config = load_yaml_config(config_file)
        config = merge_config(config, yaml_config)
        print(f"âœ“ é…ç½®åŠ è½½æˆåŠŸ")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°YAMLé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    return config


def save_config_to_yaml(config: Config, output_file: str = "train_config_generated.yaml"):
    """
    å°†é…ç½®å¯¹è±¡ä¿å­˜ä¸ºYAMLæ–‡ä»¶
    
    Args:
        config: é…ç½®å¯¹è±¡
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    yaml_dict = {
        'model': {
            'model_name_or_path': config.model.model_name_or_path,
            'use_lora': config.model.use_lora,
            'lora_r': config.model.lora_r,
            'lora_alpha': config.model.lora_alpha,
            'lora_dropout': config.model.lora_dropout,
            'lora_target_modules': config.model.lora_target_modules,
        },
        'training': {
            'output_dir': config.training.output_dir,
            'num_train_epochs': config.training.num_train_epochs,
            'per_device_train_batch_size': config.training.per_device_train_batch_size,
            'per_device_eval_batch_size': config.training.per_device_eval_batch_size,
            'gradient_accumulation_steps': config.training.gradient_accumulation_steps,
            'learning_rate': config.training.learning_rate,
            'warmup_steps': config.training.warmup_steps,
            'weight_decay': config.training.weight_decay,
            'max_grad_norm': config.training.max_grad_norm,
            'kl_beta': config.training.kl_beta,
            'kl_schedule': config.training.kl_schedule,
            'kl_beta_min': config.training.kl_beta_min,
            'kl_beta_max': config.training.kl_beta_max,
            'optimizer': config.training.optimizer,
            'lr_scheduler_type': config.training.lr_scheduler_type,
            'save_strategy': config.training.save_strategy,
            'save_steps': config.training.save_steps,
            'save_total_limit': config.training.save_total_limit,
            'logging_steps': config.training.logging_steps,
            'eval_strategy': config.training.eval_strategy,
            'eval_steps': config.training.eval_steps,
            'fp16': config.training.fp16,
            'bf16': config.training.bf16,
            'gradient_checkpointing': config.training.gradient_checkpointing,
            'seed': config.training.seed,
        },
        'data': {
            'data_dir': config.data.data_dir,
            'processed_data_file': config.data.processed_data_file,
            'max_length': config.data.max_length,
            'min_length': config.data.min_length,
            'stride': config.data.stride,
            'val_ratio': config.data.val_ratio,
            'max_files': config.data.max_files,
            'span_mask_ratio': config.data.span_mask_ratio,
            'token_mask_ratio': config.data.token_mask_ratio,
            'no_mask_ratio': config.data.no_mask_ratio,
            'span_length_range': list(config.data.span_length_range),
        },
        'evaluation': {
            'eval_batch_size': config.eval.eval_batch_size,
            'compute_perplexity': config.eval.compute_perplexity,
            'compute_ngram_overlap': config.eval.compute_ngram_overlap,
            'ngram_sizes': config.eval.ngram_sizes,
            'max_new_tokens': config.eval.max_new_tokens,
            'temperature': config.eval.temperature,
            'top_p': config.eval.top_p,
            'top_k': config.eval.top_k,
            'num_samples': config.eval.num_samples,
        }
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        yaml.dump(yaml_dict, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
    
    print(f"âœ“ é…ç½®å·²ä¿å­˜åˆ°: {output_file}")


def print_config(config: Config):
    """æ‰“å°é…ç½®ä¿¡æ¯"""
    print("\n" + "="*70)
    print("å½“å‰é…ç½®")
    print("="*70)
    
    print("\nã€æ¨¡å‹é…ç½®ã€‘")
    print(f"  æ¨¡å‹: {config.model.model_name_or_path}")
    print(f"  ä½¿ç”¨LoRA: {config.model.use_lora}")
    if config.model.use_lora:
        print(f"  LoRA Rank: {config.model.lora_r}")
        print(f"  LoRA Alpha: {config.model.lora_alpha}")
    
    print("\nã€è®­ç»ƒé…ç½®ã€‘")
    print(f"  è¾“å‡ºç›®å½•: {config.training.output_dir}")
    print(f"  è®­ç»ƒè½®æ•°: {config.training.num_train_epochs}")
    print(f"  Batch Size: {config.training.per_device_train_batch_size}")
    print(f"  æ¢¯åº¦ç´¯ç§¯: {config.training.gradient_accumulation_steps}")
    print(f"  æœ‰æ•ˆBatch Size: {config.training.per_device_train_batch_size * config.training.gradient_accumulation_steps}")
    print(f"  å­¦ä¹ ç‡: {config.training.learning_rate}")
    print(f"  KL Beta: {config.training.kl_beta}")
    
    print("\nã€æ•°æ®é…ç½®ã€‘")
    print(f"  æ•°æ®ç›®å½•: {config.data.data_dir}")
    print(f"  æœ€å¤§é•¿åº¦: {config.data.max_length}")
    print(f"  Spanæ©ç : {config.data.span_mask_ratio*100:.0f}%")
    print(f"  Tokenæ©ç : {config.data.token_mask_ratio*100:.0f}%")
    print(f"  æ— æ©ç : {config.data.no_mask_ratio*100:.0f}%")
    
    print("\nã€GPUä¼˜åŒ–ã€‘")
    print(f"  FP16: {config.training.fp16}")
    print(f"  BF16: {config.training.bf16}")
    print(f"  æ¢¯åº¦æ£€æŸ¥ç‚¹: {config.training.gradient_checkpointing}")
    
    print("="*70 + "\n")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="é…ç½®ç®¡ç†å·¥å…·")
    parser.add_argument("--load", type=str, help="åŠ è½½YAMLé…ç½®æ–‡ä»¶")
    parser.add_argument("--save", action="store_true", help="ä¿å­˜å½“å‰é…ç½®åˆ°YAML")
    parser.add_argument("--print", action="store_true", help="æ‰“å°é…ç½®")
    
    args = parser.parse_args()
    
    if args.load:
        config = load_config(args.load)
        print_config(config)
    elif args.save:
        config = Config()
        save_config_to_yaml(config)
    elif args.print:
        config = load_config()
        print_config(config)
    else:
        print("é…ç½®ç®¡ç†å·¥å…·\n")
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  # åŠ è½½é…ç½®")
        print("  python config_loader.py --load train_config.yaml")
        print("\n  # æ‰“å°å½“å‰é…ç½®")
        print("  python config_loader.py --print")
        print("\n  # ä¿å­˜é…ç½®åˆ°YAML")
        print("  python config_loader.py --save")
