# âš™ï¸ é…ç½®æŒ‡å—

## ğŸ¯ å¿«é€Ÿé…ç½®ï¼ˆæ¨èï¼‰

### æ­¥éª¤1ï¼šç¼–è¾‘é…ç½®æ–‡ä»¶

æ‰“å¼€ `train_config.yaml`ï¼Œæ‰¾åˆ°æ¨¡å‹é…ç½®éƒ¨åˆ†ï¼š

```yaml
# ============================================
# æ¨¡å‹é…ç½® - åœ¨è¿™é‡Œå®šåˆ¶æ¨¡å‹! 
# ============================================
model:
  model_name_or_path: "Qwen/Qwen2.5-7B"  # æ”¹æˆä½ æƒ³è¦çš„æ¨¡å‹
  use_peft: true                          # ä½¿ç”¨LoRAï¼ˆæ¨èï¼‰
  lora_r: 8                               # LoRAç§©ï¼ˆ4-32ï¼‰
```

### æ­¥éª¤2ï¼šè¿è¡Œè®­ç»ƒ

```bash
python train.py --config train_config.yaml
```

å°±è¿™ä¹ˆç®€å•ï¼âœ¨

---

## ğŸ“ å¸¸ç”¨æ¨¡å‹åˆ—è¡¨

### ä¸­æ–‡æ¨¡å‹ï¼ˆæ¨èï¼‰

| æ¨¡å‹ | å¤§å° | æ˜¾å­˜éœ€æ±‚ | è¯´æ˜ |
|------|------|---------|------|
| `Qwen/Qwen2.5-1.5B` | 1.5B | ~8GB | è½»é‡çº§ï¼Œé€‚åˆæµ‹è¯• |
| `Qwen/Qwen2.5-7B` | 7B | ~24GB | æ ‡å‡†é…ç½® â­ |
| `Qwen/Qwen2.5-14B` | 14B | ~48GB | å¤§æ¨¡å‹ï¼Œæ•ˆæœæ›´å¥½ |
| `THUDM/chatglm3-6b` | 6B | ~20GB | ChatGLM3 |

### è‹±æ–‡/å¤šè¯­è¨€æ¨¡å‹

| æ¨¡å‹ | å¤§å° | æ˜¾å­˜éœ€æ±‚ | è¯´æ˜ |
|------|------|---------|------|
| `meta-llama/Llama-3-8B` | 8B | ~28GB | LLaMA 3 |
| `mistralai/Mistral-7B-v0.1` | 7B | ~24GB | Mistral |

---

## ğŸ”‘ æ ¸å¿ƒå‚æ•°è¯´æ˜

### 1. KL Betaï¼ˆæœ€é‡è¦ï¼ï¼‰

æ§åˆ¶é£æ ¼è¿ç§»å¼ºåº¦ï¼š

```yaml
training:
  kl_beta: 0.1  # é»˜è®¤å€¼
```

| å€¼ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|----|------|---------|
| 0.05-0.08 | è½»åº¦é£æ ¼ | ä¿æŒé€šç”¨èƒ½åŠ›ï¼Œè½»å¾®æ­¦ä¾ é£æ ¼ |
| 0.1-0.15 | ä¸­ç­‰é£æ ¼ | å¹³è¡¡æ•ˆæœï¼Œ**æ¨è** â­ |
| 0.15-0.2 | å¼ºé£æ ¼ | æ˜æ˜¾æ­¦ä¾ å‘³ï¼Œä½†å¯èƒ½ä¸å¤Ÿæµç•… |

### 2. æ‰¹æ¬¡å¤§å°

æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼š

```yaml
training:
  per_device_train_batch_size: 4       # å•å¡æ‰¹æ¬¡å¤§å°
  gradient_accumulation_steps: 4       # æ¢¯åº¦ç´¯ç§¯
```

**æœ‰æ•ˆæ‰¹æ¬¡ = `per_device_train_batch_size` Ã— `gradient_accumulation_steps` Ã— GPUæ•°é‡**

### 3. LoRAå‚æ•°

```yaml
model:
  lora_r: 8        # LoRAç§©ï¼ˆ4/8/16/32ï¼‰
  lora_alpha: 16   # é€šå¸¸æ˜¯lora_rçš„2å€
```

- `lora_r`è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½ä½†æ˜¾å­˜è¶Šé«˜
- æ¨èï¼š8æˆ–16

### 4. å­¦ä¹ ç‡

```yaml
training:
  learning_rate: 0.0001  # 1e-4
```

- å¤ªå¤§ï¼šè®­ç»ƒä¸ç¨³å®š
- å¤ªå°ï¼šæ”¶æ•›æ…¢
- æ¨èï¼š5e-5 åˆ° 1e-4

---

## ğŸ’» æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥

### æ˜¾å­˜ä¸è¶³ï¼Ÿè¯•è¯•è¿™äº›ï¼š

| å‚æ•° | åŸå€¼ | ä¼˜åŒ–å€¼ | èŠ‚çœæ˜¾å­˜ |
|------|------|--------|---------|
| `per_device_train_batch_size` | 8 | 2 æˆ– 1 | ~70% |
| `lora_r` | 16 | 8 æˆ– 4 | ~40% |
| `max_length` | 512 | 256 | ~50% |

### æ˜¾å­˜å‚è€ƒè¡¨

| é…ç½® | æ˜¾å­˜éœ€æ±‚ | é€‚ç”¨GPU |
|------|---------|---------|
| 1.5B + LoRA4 + BS1 | ~6GB | RTX 3060 |
| 7B + LoRA8 + BS4 | ~24GB | RTX 4090 / A100 |
| 7B + LoRA16 + BS8 | ~40GB | A100 40GB |
| 14B + LoRA16 + BS4 | ~48GB | A100 80GB |

---

## ğŸ¨ å¿«é€Ÿé…ç½®é¢„è®¾

### é¢„è®¾1ï¼šå¿«é€Ÿæµ‹è¯•ï¼ˆ6GBæ˜¾å­˜ï¼‰

```yaml
model:
  model_name_or_path: "Qwen/Qwen2.5-1.5B"
  lora_r: 4

training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  num_train_epochs: 1
  kl_beta: 0.1
```

### é¢„è®¾2ï¼šæ ‡å‡†è®­ç»ƒï¼ˆ24GBæ˜¾å­˜ï¼‰â­

```yaml
model:
  model_name_or_path: "Qwen/Qwen2.5-7B"
  lora_r: 8

training:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  num_train_epochs: 3
  kl_beta: 0.1
```

### é¢„è®¾3ï¼šå¤§æ¨¡å‹è®­ç»ƒï¼ˆ48GBæ˜¾å­˜ï¼‰

```yaml
model:
  model_name_or_path: "Qwen/Qwen2.5-14B"
  lora_r: 16

training:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 8
  num_train_epochs: 5
  kl_beta: 0.12
```

---

## ğŸš€ é«˜çº§ç”¨æ³•

### å‘½ä»¤è¡Œè¦†ç›–é…ç½®

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œä½†è¦†ç›–éƒ¨åˆ†å‚æ•°
python train.py \
  --config train_config.yaml \
  --model_name Qwen/Qwen2.5-14B \
  --kl_beta 0.15 \
  --epochs 5
```

### å¤šå¡è®­ç»ƒ

```bash
# ä½¿ç”¨4å¼ GPU
torchrun --nproc_per_node=4 train.py --config train_config.yaml
```

### æŸ¥çœ‹å½“å‰é…ç½®

```bash
python -c "from config_loader import load_config, print_config; \
           config = load_config('train_config.yaml'); \
           print_config(config)"
```

---

## â“ å¸¸è§é—®é¢˜

### Q: é£æ ¼ä¸æ˜æ˜¾æ€ä¹ˆåŠï¼Ÿ
- å¢åŠ  `kl_beta`ï¼š0.1 â†’ 0.15 æˆ– 0.2
- å¢åŠ è®­ç»ƒè½®æ•°ï¼š3 â†’ 5
- æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ

### Q: è¾“å‡ºä¸æµç•…æ€ä¹ˆåŠï¼Ÿ
- å‡å° `kl_beta`ï¼š0.15 â†’ 0.1 æˆ– 0.08
- æ£€æŸ¥è®­ç»ƒæ˜¯å¦è¿‡æ‹Ÿåˆ

### Q: è®­ç»ƒå¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
- å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
- å‡å°æ•°æ®é›†å¤§å°
- ä½¿ç”¨å¤šå¡è®­ç»ƒ

### Q: å¦‚ä½•ä¸‹è½½æ¨¡å‹ï¼Ÿ
```bash
# å›½å†…ç”¨æˆ·ï¼ˆæ¨èä½¿ç”¨é•œåƒï¼‰
python scripts/download_model.py --model Qwen/Qwen2.5-7B --use-mirror

# å›½å¤–ç”¨æˆ·
python scripts/download_model.py --model Qwen/Qwen2.5-7B
```

---

## ğŸ“š å‚è€ƒ

å®Œæ•´å‚æ•°è¯´æ˜è¯·æŸ¥çœ‹ `train_config.yaml` ä¸­çš„æ³¨é‡Šã€‚

---

**æç¤º**ï¼šå¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œåªéœ€è¦ä¿®æ”¹ `model_name_or_path` å’Œ `kl_beta` è¿™ä¸¤ä¸ªå‚æ•°ï¼
