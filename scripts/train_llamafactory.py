"""
LLaMA-Factoryé›†æˆè®­ç»ƒè„šæœ¬
ä½¿ç”¨LLaMA-Factoryæ¡†æ¶è¿›è¡Œæ­¦ä¾ é£æ ¼è®­ç»ƒ
"""

import os
import sys
import json
import yaml
import subprocess
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import Config


def check_llamafactory_installed():
    """æ£€æŸ¥LLaMA-Factoryæ˜¯å¦å·²å®‰è£…"""
    try:
        import llamafactory
        print("âœ“ LLaMA-Factoryå·²å®‰è£…")
        return True
    except ImportError:
        print("âœ— LLaMA-Factoryæœªå®‰è£…")
        return False


def install_llamafactory():
    """å®‰è£…LLaMA-Factory"""
    print("\nå¼€å§‹å®‰è£…LLaMA-Factory...")
    print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...\n")
    
    try:
        # æ–¹æ³•1: ä»PyPIå®‰è£…ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        subprocess.run(["pip", "install", "llamafactory"], check=True)
        print("âœ“ LLaMA-Factoryå®‰è£…æˆåŠŸ")
        return True
    except:
        # æ–¹æ³•2: ä»GitHubå®‰è£…
        print("å°è¯•ä»GitHubå®‰è£…...")
        try:
            subprocess.run([
                "pip", "install", 
                "git+https://github.com/hiyouga/LLaMA-Factory.git"
            ], check=True)
            print("âœ“ LLaMA-Factoryå®‰è£…æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âœ— å®‰è£…å¤±è´¥: {e}")
            print("\næ‰‹åŠ¨å®‰è£…æ–¹æ³•:")
            print("1. git clone https://github.com/hiyouga/LLaMA-Factory.git")
            print("2. cd LLaMA-Factory")
            print("3. pip install -e .")
            return False


def create_dataset_info(data_dir: str = "./data", output_file: str = "./dataset_info.json"):
    """åˆ›å»ºLLaMA-Factoryçš„dataset_info.json"""
    
    dataset_info = {
        "wuxia_style": {
            "file_name": "processed_wuxia_data.jsonl",
            "formatting": "sharegpt",
            "columns": {
                "messages": "text"
            },
            "tags": {
                "role_tag": "role",
                "content_tag": "content"
            }
        }
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, indent=2, ensure_ascii=False)
    
    print(f"âœ“ æ•°æ®é›†é…ç½®å·²åˆ›å»º: {output_file}")


def create_llamafactory_config(config: Config, output_file: str = "./llamafactory_config.yaml"):
    """åˆ›å»ºLLaMA-Factoryè®­ç»ƒé…ç½®"""
    
    # æ£€æµ‹GPU
    import torch
    has_gpu = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if has_gpu else 0
    
    llamafactory_config = {
        # æ¨¡å‹é…ç½®
        "model_name_or_path": config.model.model_name_or_path,
        "trust_remote_code": True,
        
        # LoRAé…ç½®
        "finetuning_type": "lora",
        "lora_rank": config.model.lora_r,
        "lora_alpha": config.model.lora_alpha,
        "lora_dropout": config.model.lora_dropout,
        "lora_target": ",".join(config.model.lora_target_modules),
        
        # æ•°æ®é…ç½®
        "dataset": "wuxia_style",
        "dataset_dir": ".",
        "cutoff_len": config.data.max_length,
        "val_size": config.data.val_ratio,
        "overwrite_cache": True,
        "preprocessing_num_workers": 4,
        
        # è®­ç»ƒé…ç½®
        "output_dir": config.training.output_dir,
        "num_train_epochs": config.training.num_train_epochs,
        "per_device_train_batch_size": config.training.per_device_train_batch_size,
        "gradient_accumulation_steps": config.training.gradient_accumulation_steps,
        "learning_rate": config.training.learning_rate,
        "warmup_steps": config.training.warmup_steps,
        "weight_decay": config.training.weight_decay,
        "max_grad_norm": config.training.max_grad_norm,
        "lr_scheduler_type": config.training.lr_scheduler_type,
        
        # ä¼˜åŒ–å™¨
        "optim": "adamw_torch",
        "adam_beta1": config.training.adam_beta1,
        "adam_beta2": config.training.adam_beta2,
        
        # ä¿å­˜å’Œæ—¥å¿—
        "save_strategy": config.training.save_strategy,
        "save_steps": config.training.save_steps,
        "save_total_limit": config.training.save_total_limit,
        "logging_steps": config.training.logging_steps,
        "eval_strategy": config.training.eval_strategy,
        "eval_steps": config.training.eval_steps,
        
        # GPUä¼˜åŒ–
        "fp16": config.training.fp16 and has_gpu,
        "bf16": config.training.bf16 and has_gpu,
        "gradient_checkpointing": config.training.gradient_checkpointing,
        "ddp_timeout": 180000000,
        
        # ç‰¹æ®Šé…ç½®ï¼ˆæ­¦ä¾ é£æ ¼å¯¹é½ï¼‰
        "plot_loss": True,
        "overwrite_output_dir": True,
        
        # GPUé…ç½®
        "device_map": "auto" if has_gpu else "cpu",
    }
    
    # å¦‚æœæœ‰å¤šGPUï¼Œå¯ç”¨DDP
    if gpu_count > 1:
        llamafactory_config["ddp_find_unused_parameters"] = False
        llamafactory_config["fsdp"] = ""
    
    with open(output_file, 'w', encoding='utf-8') as f:
        yaml.dump(llamafactory_config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"âœ“ LLaMA-Factoryé…ç½®å·²åˆ›å»º: {output_file}")
    print(f"\nGPUé…ç½®:")
    print(f"  å¯ç”¨: {'æ˜¯' if has_gpu else 'å¦'}")
    if has_gpu:
        print(f"  æ•°é‡: {gpu_count}")
        print(f"  è®¾å¤‡: {[torch.cuda.get_device_name(i) for i in range(gpu_count)]}")
        print(f"  æ˜¾å­˜: {[f'{torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB' for i in range(gpu_count)]}")
    
    return llamafactory_config


def prepare_data_for_llamafactory():
    """å‡†å¤‡æ•°æ®æ ¼å¼ä»¥é€‚é…LLaMA-Factory"""
    
    input_file = "processed_wuxia_data.jsonl"
    output_file = "processed_wuxia_data_llama.jsonl"
    
    if not os.path.exists(input_file):
        print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†:")
        print("  python train.py  # ä¼šè‡ªåŠ¨å¤„ç†æ•°æ®")
        return False
    
    print(f"\nè½¬æ¢æ•°æ®æ ¼å¼...")
    
    # è½¬æ¢ä¸ºLLaMA-Factoryæ ¼å¼
    converted = 0
    with open(input_file, 'r', encoding='utf-8') as fin, \
         open(output_file, 'w', encoding='utf-8') as fout:
        
        for line in fin:
            data = json.loads(line)
            
            # è½¬æ¢ä¸ºShareGPTæ ¼å¼ï¼ˆç”¨äºé¢„è®­ç»ƒï¼‰
            converted_data = {
                "messages": [
                    {
                        "role": "user",
                        "content": data['text'][:50]  # å‰50å­—ä½œä¸ºprompt
                    },
                    {
                        "role": "assistant",
                        "content": data['text']  # å®Œæ•´æ–‡æœ¬ä½œä¸ºresponse
                    }
                ]
            }
            
            fout.write(json.dumps(converted_data, ensure_ascii=False) + '\n')
            converted += 1
    
    print(f"âœ“ æ•°æ®è½¬æ¢å®Œæˆ: {converted} æ¡")
    print(f"  è¾“å‡º: {output_file}")
    
    return True


def train_with_llamafactory(config_file: str = "./llamafactory_config.yaml", use_cli: bool = True):
    """ä½¿ç”¨LLaMA-Factoryè¿›è¡Œè®­ç»ƒ"""
    
    print("\n" + "="*70)
    print("ğŸš€ å¼€å§‹LLaMA-Factoryè®­ç»ƒ")
    print("="*70)
    
    if use_cli:
        # ä½¿ç”¨CLIæ–¹å¼ï¼ˆæ¨èï¼‰
        cmd = [
            "llamafactory-cli", "train",
            config_file
        ]
        
        print(f"\næ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
        
        try:
            subprocess.run(cmd, check=True)
            print("\nâœ“ è®­ç»ƒå®Œæˆï¼")
            return True
        except subprocess.CalledProcessError as e:
            print(f"\nâœ— è®­ç»ƒå¤±è´¥: {e}")
            return False
    else:
        # ä½¿ç”¨Python API
        try:
            from llamafactory.train import run_exp
            
            # è¯»å–é…ç½®
            with open(config_file, 'r', encoding='utf-8') as f:
                config_dict = yaml.safe_load(f)
            
            # è¿è¡Œè®­ç»ƒ
            run_exp(config_dict)
            
            print("\nâœ“ è®­ç»ƒå®Œæˆï¼")
            return True
        except Exception as e:
            print(f"\nâœ— è®­ç»ƒå¤±è´¥: {e}")
            return False


def export_model(checkpoint_dir: str, output_dir: str):
    """å¯¼å‡ºè®­ç»ƒå¥½çš„æ¨¡å‹"""
    
    print(f"\nå¯¼å‡ºæ¨¡å‹...")
    print(f"  ä»: {checkpoint_dir}")
    print(f"  åˆ°: {output_dir}")
    
    cmd = [
        "llamafactory-cli", "export",
        "--model_name_or_path", checkpoint_dir,
        "--output_dir", output_dir,
        "--export_dir", output_dir,
    ]
    
    try:
        subprocess.run(cmd, check=True)
        print(f"âœ“ æ¨¡å‹å¯¼å‡ºæˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— å¯¼å‡ºå¤±è´¥: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description="LLaMA-Factoryé›†æˆè®­ç»ƒ")
    parser.add_argument("--install", action="store_true", help="å®‰è£…LLaMA-Factory")
    parser.add_argument("--prepare", action="store_true", help="å‡†å¤‡æ•°æ®å’Œé…ç½®")
    parser.add_argument("--train", action="store_true", help="å¼€å§‹è®­ç»ƒ")
    parser.add_argument("--config_file", type=str, default="./llamafactory_config.yaml", help="é…ç½®æ–‡ä»¶")
    parser.add_argument("--export", type=str, help="å¯¼å‡ºæ¨¡å‹åˆ°æŒ‡å®šç›®å½•")
    parser.add_argument("--checkpoint", type=str, help="checkpointç›®å½•")
    
    args = parser.parse_args()
    
    # å®‰è£…LLaMA-Factory
    if args.install:
        install_llamafactory()
        return
    
    # å‡†å¤‡æ•°æ®å’Œé…ç½®
    if args.prepare:
        print("\n" + "="*70)
        print("å‡†å¤‡LLaMA-Factoryè®­ç»ƒç¯å¢ƒ")
        print("="*70)
        
        # æ£€æŸ¥å®‰è£…
        if not check_llamafactory_installed():
            print("\nè¯·å…ˆå®‰è£…LLaMA-Factory:")
            print("  python train_llamafactory.py --install")
            return
        
        # åŠ è½½é…ç½®
        config = Config()
        
        # åˆ›å»ºæ•°æ®é›†é…ç½®
        create_dataset_info()
        
        # è½¬æ¢æ•°æ®æ ¼å¼
        prepare_data_for_llamafactory()
        
        # åˆ›å»ºè®­ç»ƒé…ç½®
        create_llamafactory_config(config, args.config_file)
        
        print("\nâœ“ å‡†å¤‡å®Œæˆï¼")
        print("\nä¸‹ä¸€æ­¥:")
        print(f"  python train_llamafactory.py --train --config_file {args.config_file}")
        
        return
    
    # è®­ç»ƒ
    if args.train:
        if not os.path.exists(args.config_file):
            print(f"âœ— é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config_file}")
            print("è¯·å…ˆè¿è¡Œ: python train_llamafactory.py --prepare")
            return
        
        success = train_with_llamafactory(args.config_file)
        
        if success:
            print("\næ¨¡å‹å·²ä¿å­˜ï¼")
            print("\nä½¿ç”¨æ¨¡å‹:")
            print("  python inference.py --model_path ./output/wuxia_model")
        
        return
    
    # å¯¼å‡ºæ¨¡å‹
    if args.export:
        if not args.checkpoint:
            print("âœ— è¯·æŒ‡å®šcheckpointç›®å½•: --checkpoint")
            return
        
        export_model(args.checkpoint, args.export)
        return
    
    # é»˜è®¤ï¼šæ˜¾ç¤ºå¸®åŠ©
    print("LLaMA-Factoryé›†æˆè®­ç»ƒå·¥å…·\n")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("\n1. å®‰è£…LLaMA-Factory")
    print("   python train_llamafactory.py --install")
    print("\n2. å‡†å¤‡æ•°æ®å’Œé…ç½®")
    print("   python train_llamafactory.py --prepare")
    print("\n3. å¼€å§‹è®­ç»ƒ")
    print("   python train_llamafactory.py --train")
    print("\n4. å¯¼å‡ºæ¨¡å‹")
    print("   python train_llamafactory.py --export ./output/final --checkpoint ./output/checkpoint-1000")


if __name__ == "__main__":
    main()
