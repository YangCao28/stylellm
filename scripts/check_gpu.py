"""
GPUè®­ç»ƒé…ç½®å’Œæ£€æµ‹è„šæœ¬
æ£€æµ‹GPUçŠ¶æ€ï¼Œä¼˜åŒ–è®­ç»ƒé…ç½®
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import subprocess
import json
from typing import Dict, List
import platform


def check_gpu_availability() -> Dict:
    """æ£€æµ‹GPUå¯ç”¨æ€§å’Œè¯¦ç»†ä¿¡æ¯"""
    
    info = {
        "cuda_available": torch.cuda.is_available(),
        "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
        "pytorch_version": torch.__version__,
        "device_count": 0,
        "devices": [],
        "total_memory_gb": 0,
        "recommended_batch_size": 1,
    }
    
    if info["cuda_available"]:
        info["device_count"] = torch.cuda.device_count()
        
        for i in range(info["device_count"]):
            props = torch.cuda.get_device_properties(i)
            device_info = {
                "id": i,
                "name": props.name,
                "compute_capability": f"{props.major}.{props.minor}",
                "total_memory_gb": props.total_memory / 1024**3,
                "multi_processor_count": props.multi_processor_count,
            }
            
            # è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                mem_allocated = torch.cuda.memory_allocated(i) / 1024**3
                mem_reserved = torch.cuda.memory_reserved(i) / 1024**3
                mem_free = device_info["total_memory_gb"] - mem_reserved
                
                device_info["memory_allocated_gb"] = mem_allocated
                device_info["memory_reserved_gb"] = mem_reserved
                device_info["memory_free_gb"] = mem_free
            
            info["devices"].append(device_info)
            info["total_memory_gb"] += device_info["total_memory_gb"]
        
        # æ ¹æ®æ˜¾å­˜æ¨èbatch size
        if info["total_memory_gb"] < 12:
            info["recommended_batch_size"] = 1
        elif info["total_memory_gb"] < 24:
            info["recommended_batch_size"] = 2
        elif info["total_memory_gb"] < 48:
            info["recommended_batch_size"] = 4
        else:
            info["recommended_batch_size"] = 8
    
    return info


def check_cudnn():
    """æ£€æŸ¥cuDNN"""
    cudnn_available = torch.backends.cudnn.is_available()
    cudnn_version = torch.backends.cudnn.version() if cudnn_available else None
    
    return {
        "available": cudnn_available,
        "version": cudnn_version,
        "enabled": torch.backends.cudnn.enabled,
    }


def check_nvidia_smi():
    """ä½¿ç”¨nvidia-smiè·å–è¯¦ç»†GPUä¿¡æ¯"""
    try:
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=index,name,driver_version,memory.total,memory.free,memory.used,temperature.gpu,utilization.gpu", 
             "--format=csv,noheader,nounits"],
            capture_output=True,
            text=True,
            check=True
        )
        
        devices = []
        for line in result.stdout.strip().split('\n'):
            parts = [p.strip() for p in line.split(',')]
            if len(parts) >= 8:
                devices.append({
                    "index": int(parts[0]),
                    "name": parts[1],
                    "driver_version": parts[2],
                    "memory_total_mb": float(parts[3]),
                    "memory_free_mb": float(parts[4]),
                    "memory_used_mb": float(parts[5]),
                    "temperature_c": int(parts[6]) if parts[6] else 0,
                    "utilization_percent": int(parts[7]) if parts[7] else 0,
                })
        
        return {"available": True, "devices": devices}
    
    except (subprocess.CalledProcessError, FileNotFoundError):
        return {"available": False, "devices": []}


def recommend_training_config(gpu_info: Dict) -> Dict:
    """æ ¹æ®GPUé…ç½®æ¨èè®­ç»ƒå‚æ•°"""
    
    if not gpu_info["cuda_available"]:
        return {
            "device": "cpu",
            "batch_size": 1,
            "gradient_accumulation_steps": 8,
            "fp16": False,
            "bf16": False,
            "gradient_checkpointing": True,
            "use_lora": True,
            "lora_r": 8,
            "warning": "âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œè®­ç»ƒé€Ÿåº¦ä¼šéå¸¸æ…¢ï¼",
        }
    
    total_memory = gpu_info["total_memory_gb"]
    device_count = gpu_info["device_count"]
    
    # æ ¹æ®æ˜¾å­˜æ¨èé…ç½®
    if total_memory < 12:  # <12GB
        config = {
            "device": "cuda",
            "batch_size": 1,
            "gradient_accumulation_steps": 16,
            "fp16": True,
            "bf16": False,
            "gradient_checkpointing": True,
            "use_lora": True,
            "lora_r": 8,
            "use_4bit": True,
            "max_length": 256,
            "warning": "âš ï¸  æ˜¾å­˜è¾ƒå°ï¼Œå»ºè®®ä½¿ç”¨4-bité‡åŒ–",
        }
    
    elif total_memory < 24:  # 12-24GB
        config = {
            "device": "cuda",
            "batch_size": 2,
            "gradient_accumulation_steps": 8,
            "fp16": True,
            "bf16": False,
            "gradient_checkpointing": True,
            "use_lora": True,
            "lora_r": 16,
            "use_4bit": False,
            "max_length": 512,
            "note": "âœ“ é…ç½®è‰¯å¥½ï¼Œå¯æ­£å¸¸è®­ç»ƒ7Bæ¨¡å‹",
        }
    
    elif total_memory < 48:  # 24-48GB
        config = {
            "device": "cuda",
            "batch_size": 4,
            "gradient_accumulation_steps": 4,
            "fp16": True,
            "bf16": True,  # A100æ”¯æŒBF16
            "gradient_checkpointing": False,
            "use_lora": True,
            "lora_r": 32,
            "use_4bit": False,
            "max_length": 1024,
            "note": "âœ“ é…ç½®ä¼˜ç§€ï¼Œå¯è®­ç»ƒæ›´å¤§æ¨¡å‹",
        }
    
    else:  # >48GB
        config = {
            "device": "cuda",
            "batch_size": 8,
            "gradient_accumulation_steps": 2,
            "fp16": False,
            "bf16": True,
            "gradient_checkpointing": False,
            "use_lora": True,
            "lora_r": 64,
            "use_4bit": False,
            "max_length": 2048,
            "note": "âœ“ é…ç½®é¡¶çº§ï¼Œå¯è¿›è¡Œå…¨å‚æ•°å¾®è°ƒ",
        }
    
    # å¤šGPUé…ç½®
    if device_count > 1:
        config["use_ddp"] = True
        config["device_count"] = device_count
        config["note"] = f"âœ“ æ£€æµ‹åˆ°{device_count}å—GPUï¼Œå°†ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ"
    
    return config


def test_gpu_performance():
    """æµ‹è¯•GPUæ€§èƒ½"""
    if not torch.cuda.is_available():
        print("âŒ æ²¡æœ‰å¯ç”¨çš„GPU")
        return
    
    print("\n" + "="*70)
    print("ğŸ”¥ GPUæ€§èƒ½æµ‹è¯•")
    print("="*70)
    
    device = torch.device("cuda:0")
    
    # æµ‹è¯•1: çŸ©é˜µä¹˜æ³•
    print("\n[1/3] æµ‹è¯•çŸ©é˜µä¹˜æ³•...")
    size = 8192
    a = torch.randn(size, size, device=device)
    b = torch.randn(size, size, device=device)
    
    import time
    torch.cuda.synchronize()
    start = time.time()
    c = torch.matmul(a, b)
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    tflops = (2 * size ** 3) / elapsed / 1e12
    print(f"  çŸ©é˜µå¤§å°: {size}x{size}")
    print(f"  è€—æ—¶: {elapsed:.4f}ç§’")
    print(f"  æ€§èƒ½: {tflops:.2f} TFLOPS")
    
    # æµ‹è¯•2: æ˜¾å­˜å¸¦å®½
    print("\n[2/3] æµ‹è¯•æ˜¾å­˜å¸¦å®½...")
    size = 100 * 1024 * 1024  # 100M elements
    data = torch.randn(size, device=device)
    
    torch.cuda.synchronize()
    start = time.time()
    result = data * 2
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    bandwidth = (size * 4 * 2) / elapsed / 1e9  # 4 bytes per float, read+write
    print(f"  æ•°æ®é‡: {size / 1024 / 1024:.2f} M elements")
    print(f"  è€—æ—¶: {elapsed:.4f}ç§’")
    print(f"  å¸¦å®½: {bandwidth:.2f} GB/s")
    
    # æµ‹è¯•3: æ··åˆç²¾åº¦
    print("\n[3/3] æµ‹è¯•æ··åˆç²¾åº¦...")
    model = torch.nn.Linear(4096, 4096).to(device)
    x = torch.randn(128, 4096, device=device)
    
    # FP32
    torch.cuda.synchronize()
    start = time.time()
    with torch.no_grad():
        for _ in range(100):
            y = model(x)
    torch.cuda.synchronize()
    fp32_time = time.time() - start
    
    # FP16
    model_fp16 = model.half()
    x_fp16 = x.half()
    torch.cuda.synchronize()
    start = time.time()
    with torch.no_grad():
        for _ in range(100):
            y = model_fp16(x_fp16)
    torch.cuda.synchronize()
    fp16_time = time.time() - start
    
    print(f"  FP32: {fp32_time:.4f}ç§’")
    print(f"  FP16: {fp16_time:.4f}ç§’")
    print(f"  åŠ é€Ÿæ¯”: {fp32_time/fp16_time:.2f}x")
    
    print("\nâœ“ æ€§èƒ½æµ‹è¯•å®Œæˆ")


def print_gpu_info():
    """æ‰“å°è¯¦ç»†çš„GPUä¿¡æ¯"""
    
    print("\n" + "="*70)
    print("ğŸ–¥ï¸  GPUé…ç½®æ£€æµ‹")
    print("="*70)
    
    # ç³»ç»Ÿä¿¡æ¯
    print(f"\nç³»ç»Ÿä¿¡æ¯:")
    print(f"  æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"  Pythonç‰ˆæœ¬: {platform.python_version()}")
    print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # GPUä¿¡æ¯
    gpu_info = check_gpu_availability()
    
    print(f"\nCUDAä¿¡æ¯:")
    print(f"  CUDAå¯ç”¨: {'æ˜¯' if gpu_info['cuda_available'] else 'å¦'}")
    if gpu_info["cuda_available"]:
        print(f"  CUDAç‰ˆæœ¬: {gpu_info['cuda_version']}")
        print(f"  GPUæ•°é‡: {gpu_info['device_count']}")
        print(f"  æ€»æ˜¾å­˜: {gpu_info['total_memory_gb']:.2f} GB")
    
    # cuDNNä¿¡æ¯
    cudnn_info = check_cudnn()
    print(f"\ncuDNNä¿¡æ¯:")
    print(f"  å¯ç”¨: {'æ˜¯' if cudnn_info['available'] else 'å¦'}")
    if cudnn_info["available"]:
        print(f"  ç‰ˆæœ¬: {cudnn_info['version']}")
    
    # è¯¦ç»†GPUä¿¡æ¯
    if gpu_info["cuda_available"]:
        print(f"\nGPUè®¾å¤‡:")
        for device in gpu_info["devices"]:
            print(f"\n  GPU {device['id']}: {device['name']}")
            print(f"    è®¡ç®—èƒ½åŠ›: {device['compute_capability']}")
            print(f"    æ€»æ˜¾å­˜: {device['total_memory_gb']:.2f} GB")
            if "memory_free_gb" in device:
                print(f"    å·²åˆ†é…: {device['memory_allocated_gb']:.2f} GB")
                print(f"    å¯ç”¨: {device['memory_free_gb']:.2f} GB")
    
    # nvidia-smiä¿¡æ¯
    nvidia_info = check_nvidia_smi()
    if nvidia_info["available"]:
        print(f"\nnvidia-smiä¿¡æ¯:")
        for device in nvidia_info["devices"]:
            print(f"\n  GPU {device['index']}: {device['name']}")
            print(f"    é©±åŠ¨ç‰ˆæœ¬: {device['driver_version']}")
            print(f"    æ˜¾å­˜: {device['memory_used_mb']:.0f}/{device['memory_total_mb']:.0f} MB")
            print(f"    æ¸©åº¦: {device['temperature_c']}Â°C")
            print(f"    åˆ©ç”¨ç‡: {device['utilization_percent']}%")
    
    # æ¨èé…ç½®
    print("\n" + "="*70)
    print("ğŸ’¡ æ¨èè®­ç»ƒé…ç½®")
    print("="*70)
    
    recommended = recommend_training_config(gpu_info)
    
    print("\nåŸºç¡€é…ç½®:")
    print(f"  è®¾å¤‡: {recommended.get('device', 'cpu')}")
    print(f"  Batch Size: {recommended.get('batch_size', 1)}")
    print(f"  æ¢¯åº¦ç´¯ç§¯: {recommended.get('gradient_accumulation_steps', 1)}")
    print(f"  æœ‰æ•ˆBatch Size: {recommended['batch_size'] * recommended['gradient_accumulation_steps']}")
    
    print("\nä¼˜åŒ–é…ç½®:")
    print(f"  FP16: {'æ˜¯' if recommended.get('fp16', False) else 'å¦'}")
    print(f"  BF16: {'æ˜¯' if recommended.get('bf16', False) else 'å¦'}")
    print(f"  æ¢¯åº¦æ£€æŸ¥ç‚¹: {'æ˜¯' if recommended.get('gradient_checkpointing', False) else 'å¦'}")
    print(f"  4-bité‡åŒ–: {'æ˜¯' if recommended.get('use_4bit', False) else 'å¦'}")
    
    print("\nLoRAé…ç½®:")
    print(f"  ä½¿ç”¨LoRA: {'æ˜¯' if recommended.get('use_lora', True) else 'å¦'}")
    print(f"  LoRA Rank: {recommended.get('lora_r', 16)}")
    print(f"  æœ€å¤§é•¿åº¦: {recommended.get('max_length', 512)}")
    
    if "warning" in recommended:
        print(f"\n{recommended['warning']}")
    if "note" in recommended:
        print(f"\n{recommended['note']}")
    
    # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    output_file = "gpu_config.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "gpu_info": gpu_info,
            "cudnn_info": cudnn_info,
            "recommended_config": recommended
        }, f, indent=2)
    
    print(f"\né…ç½®å·²ä¿å­˜åˆ°: {output_file}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="GPUé…ç½®æ£€æµ‹å’Œä¼˜åŒ–")
    parser.add_argument("--test", action="store_true", help="è¿è¡Œæ€§èƒ½æµ‹è¯•")
    parser.add_argument("--export", type=str, help="å¯¼å‡ºé…ç½®åˆ°æ–‡ä»¶")
    
    args = parser.parse_args()
    
    # æ‰“å°GPUä¿¡æ¯
    print_gpu_info()
    
    # æ€§èƒ½æµ‹è¯•
    if args.test:
        test_gpu_performance()
    
    # å¯¼å‡ºé…ç½®
    if args.export:
        gpu_info = check_gpu_availability()
        recommended = recommend_training_config(gpu_info)
        
        with open(args.export, 'w', encoding='utf-8') as f:
            json.dump(recommended, f, indent=2)
        
        print(f"\né…ç½®å·²å¯¼å‡ºåˆ°: {args.export}")


if __name__ == "__main__":
    main()
