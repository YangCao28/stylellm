"""
ä¸‹è½½åŸºåº§æ¨¡å‹è„šæœ¬
ä»HuggingFaceä¸‹è½½config.pyä¸­å®šä¹‰çš„æ¨¡å‹åˆ°æœ¬åœ°
"""

import os
import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from transformers import AutoTokenizer, AutoModelForCausalLM
from config import Config


def download_model(model_name: str, save_dir: str = "./models", use_mirror: bool = False):
    """
    ä»HuggingFaceä¸‹è½½æ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ï¼šQwen/Qwen2.5-7Bï¼‰
        save_dir: ä¿å­˜ç›®å½•
        use_mirror: æ˜¯å¦ä½¿ç”¨é•œåƒç«™ï¼ˆå›½å†…ç”¨æˆ·æ¨èï¼‰
    """
    print("="*70)
    print("ğŸ”½ ä¸‹è½½åŸºåº§æ¨¡å‹")
    print("="*70)
    print(f"\næ¨¡å‹: {model_name}")
    print(f"ä¿å­˜åˆ°: {save_dir}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_path = Path(save_dir) / model_name.replace("/", "_")
    save_path.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®é•œåƒï¼ˆå›½å†…ç”¨æˆ·ï¼‰
    if use_mirror:
        print("\nä½¿ç”¨HuggingFaceé•œåƒç«™ï¼ˆå›½å†…åŠ é€Ÿï¼‰...")
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    
    try:
        # ä¸‹è½½tokenizer
        print("\n[1/2] ä¸‹è½½Tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            cache_dir=save_path
        )
        tokenizer.save_pretrained(save_path)
        print(f"âœ“ Tokenizerå·²ä¿å­˜")
        
        # ä¸‹è½½æ¨¡å‹
        print("\n[2/2] ä¸‹è½½æ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            cache_dir=save_path,
            torch_dtype="auto",  # è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
        )
        model.save_pretrained(save_path)
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜")
        
        print("\n" + "="*70)
        print("âœ… ä¸‹è½½å®Œæˆï¼")
        print("="*70)
        print(f"\næ¨¡å‹ä¿å­˜åœ¨: {save_path}")
        print(f"\nä½¿ç”¨æ–¹æ³•:")
        print(f"  python train.py --model_name {save_path}")
        
        return str(save_path)
        
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. ä½¿ç”¨é•œåƒç«™: python download_model.py --use_mirror")
        print("3. ä½¿ç”¨VPN")
        print("4. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶")
        return None


def download_multiple_models(models: list, save_dir: str = "./models", use_mirror: bool = False):
    """æ‰¹é‡ä¸‹è½½å¤šä¸ªæ¨¡å‹"""
    print(f"\nå‡†å¤‡ä¸‹è½½ {len(models)} ä¸ªæ¨¡å‹...\n")
    
    results = {}
    for i, model_name in enumerate(models, 1):
        print(f"\n{'='*70}")
        print(f"ä¸‹è½½è¿›åº¦: [{i}/{len(models)}]")
        print(f"{'='*70}")
        
        result = download_model(model_name, save_dir, use_mirror)
        results[model_name] = result
        
        if result:
            print(f"âœ“ {model_name} ä¸‹è½½æˆåŠŸ")
        else:
            print(f"âœ— {model_name} ä¸‹è½½å¤±è´¥")
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“Š ä¸‹è½½æ€»ç»“")
    print("="*70)
    
    success_count = sum(1 for v in results.values() if v)
    print(f"\næˆåŠŸ: {success_count}/{len(models)}")
    
    print("\næˆåŠŸä¸‹è½½çš„æ¨¡å‹:")
    for model, path in results.items():
        if path:
            print(f"  âœ“ {model}")
            print(f"    â†’ {path}")
    
    failed = [model for model, path in results.items() if not path]
    if failed:
        print("\nå¤±è´¥çš„æ¨¡å‹:")
        for model in failed:
            print(f"  âœ— {model}")


def list_recommended_models():
    """åˆ—å‡ºæ¨èçš„æ¨¡å‹"""
    print("\n" + "="*70)
    print("ğŸ“š æ¨èçš„æ­¦ä¾ é£æ ¼è®­ç»ƒæ¨¡å‹")
    print("="*70)
    
    models = {
        "æµ‹è¯•ç”¨ï¼ˆå°æ¨¡å‹ï¼‰": [
            ("gpt2", "124M", "è‹±æ–‡ï¼Œå¿«é€Ÿæµ‹è¯•"),
            ("uer/gpt2-chinese-cluecorpussmall", "124M", "ä¸­æ–‡GPT2"),
        ],
        "ç”Ÿäº§ç”¨ï¼ˆ7Bçº§ï¼‰": [
            ("Qwen/Qwen2.5-7B", "7B", "é€šä¹‰åƒé—®ï¼Œæ€§èƒ½ä¼˜ç§€"),
            ("meta-llama/Llama-3.1-8B", "8B", "LLaMA3.1ï¼Œéœ€ç”³è¯·"),
            ("01-ai/Yi-1.5-9B", "9B", "é›¶ä¸€ä¸‡ç‰©ï¼Œä¸­æ–‡å‹å¥½"),
        ],
        "è½»é‡çº§ï¼ˆ1-3Bï¼‰": [
            ("Qwen/Qwen2.5-1.5B", "1.5B", "æ˜¾å­˜å‹å¥½"),
            ("TinyLlama/TinyLlama-1.1B-Chat-v1.0", "1.1B", "è¶…è½»é‡"),
        ],
    }
    
    for category, model_list in models.items():
        print(f"\nã€{category}ã€‘")
        for name, size, desc in model_list:
            print(f"  â€¢ {name}")
            print(f"    å¤§å°: {size}, {desc}")


def check_disk_space(required_gb: float = 20):
    """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
    import shutil
    
    total, used, free = shutil.disk_usage(".")
    free_gb = free / (1024**3)
    
    print(f"\nç£ç›˜ç©ºé—´æ£€æŸ¥:")
    print(f"  å¯ç”¨ç©ºé—´: {free_gb:.2f} GB")
    print(f"  å»ºè®®ç©ºé—´: {required_gb} GB")
    
    if free_gb < required_gb:
        print(f"  âš ï¸  è­¦å‘Š: ç£ç›˜ç©ºé—´å¯èƒ½ä¸è¶³ï¼")
        return False
    else:
        print(f"  âœ“ ç©ºé—´å……è¶³")
        return True


def main():
    parser = argparse.ArgumentParser(description="ä»HuggingFaceä¸‹è½½åŸºåº§æ¨¡å‹")
    parser.add_argument("--model_name", type=str, help="æ¨¡å‹åç§°")
    parser.add_argument("--save_dir", type=str, default="./models", help="ä¿å­˜ç›®å½•")
    parser.add_argument("--use_mirror", action="store_true", help="ä½¿ç”¨é•œåƒç«™ï¼ˆå›½å†…åŠ é€Ÿï¼‰")
    parser.add_argument("--list_models", action="store_true", help="åˆ—å‡ºæ¨èæ¨¡å‹")
    parser.add_argument("--use_config", action="store_true", help="ä½¿ç”¨config.pyä¸­çš„æ¨¡å‹")
    parser.add_argument("--batch", nargs="+", help="æ‰¹é‡ä¸‹è½½å¤šä¸ªæ¨¡å‹")
    parser.add_argument("--check_space", action="store_true", help="æ£€æŸ¥ç£ç›˜ç©ºé—´")
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ¨èæ¨¡å‹
    if args.list_models:
        list_recommended_models()
        return
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    if args.check_space:
        check_disk_space()
        return
    
    # ä»configè¯»å–
    if args.use_config:
        config = Config()
        model_name = config.model.model_name_or_path
        print(f"ä½¿ç”¨config.pyä¸­çš„æ¨¡å‹: {model_name}")
        download_model(model_name, args.save_dir, args.use_mirror)
        return
    
    # æ‰¹é‡ä¸‹è½½
    if args.batch:
        download_multiple_models(args.batch, args.save_dir, args.use_mirror)
        return
    
    # å•ä¸ªä¸‹è½½
    if args.model_name:
        check_disk_space()
        download_model(args.model_name, args.save_dir, args.use_mirror)
    else:
        print("è¯·æŒ‡å®šè¦ä¸‹è½½çš„æ¨¡å‹ï¼\n")
        print("ä½¿ç”¨ç¤ºä¾‹:")
        print("  # ä¸‹è½½å•ä¸ªæ¨¡å‹")
        print("  python download_model.py --model_name Qwen/Qwen2.5-7B")
        print("\n  # ä½¿ç”¨é•œåƒç«™ï¼ˆå›½å†…ç”¨æˆ·ï¼‰")
        print("  python download_model.py --model_name Qwen/Qwen2.5-7B --use_mirror")
        print("\n  # ä½¿ç”¨config.pyä¸­çš„æ¨¡å‹")
        print("  python download_model.py --use_config --use_mirror")
        print("\n  # æ‰¹é‡ä¸‹è½½")
        print("  python download_model.py --batch gpt2 Qwen/Qwen2.5-1.5B")
        print("\n  # æŸ¥çœ‹æ¨èæ¨¡å‹")
        print("  python download_model.py --list_models")


if __name__ == "__main__":
    main()
