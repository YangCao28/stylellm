"""
æ¨ç†ç¤ºä¾‹è„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„æ­¦ä¾ é£æ ¼æ¨¡å‹ç”Ÿæˆæ–‡æœ¬
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import argparse


def load_model(model_path: str):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    
    print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    return model, tokenizer


def generate_text(
    model,
    tokenizer,
    prompt: str,
    max_length: int = 200,
    temperature: float = 0.9,
    top_p: float = 0.9,
    top_k: int = 50,
    num_samples: int = 1,
):
    """ç”Ÿæˆæ­¦ä¾ é£æ ¼æ–‡æœ¬"""
    
    # Tokenizeè¾“å…¥
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)
    
    print(f"\næç¤ºè¯: {prompt}")
    print(f"ç”Ÿæˆå‚æ•°: temp={temperature}, top_p={top_p}, top_k={top_k}")
    print("-" * 60)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            max_length=max_length,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            do_sample=True,
            num_return_sequences=num_samples,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # è§£ç è¾“å‡º
    generated_texts = []
    for i, output in enumerate(outputs):
        text = tokenizer.decode(output, skip_special_tokens=True)
        generated_texts.append(text)
        
        print(f"\næ ·æœ¬ {i+1}:")
        print(text)
        print("-" * 60)
    
    return generated_texts


def interactive_mode(model, tokenizer):
    """äº¤äº’å¼ç”Ÿæˆæ¨¡å¼"""
    print("\n" + "="*60)
    print("ğŸ—¡ï¸  æ­¦ä¾ é£æ ¼æ–‡æœ¬ç”Ÿæˆ - äº¤äº’æ¨¡å¼")
    print("="*60)
    print("\nè¾“å…¥æç¤ºè¯ï¼Œæ¨¡å‹å°†ç”Ÿæˆæ­¦ä¾ é£æ ¼çš„ç»­å†™ã€‚")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºã€‚")
    print("è¾“å…¥ 'config' è°ƒæ•´ç”Ÿæˆå‚æ•°ã€‚")
    
    # é»˜è®¤å‚æ•°
    config = {
        'max_length': 200,
        'temperature': 0.9,
        'top_p': 0.9,
        'top_k': 50,
        'num_samples': 1,
    }
    
    while True:
        print("\n" + "-"*60)
        prompt = input("æç¤ºè¯ >>> ").strip()
        
        if prompt.lower() in ['quit', 'exit', 'q']:
            print("\nå†è§ï¼æ±Ÿæ¹–å†è§ï¼âš”ï¸")
            break
        
        if prompt.lower() == 'config':
            print("\nå½“å‰é…ç½®:")
            for key, value in config.items():
                print(f"  {key}: {value}")
            
            print("\nä¿®æ”¹é…ç½®ï¼ˆç›´æ¥Enterè·³è¿‡ï¼‰:")
            for key in config.keys():
                new_value = input(f"  {key} [{config[key]}]: ").strip()
                if new_value:
                    try:
                        if key in ['max_length', 'top_k', 'num_samples']:
                            config[key] = int(new_value)
                        else:
                            config[key] = float(new_value)
                    except ValueError:
                        print(f"    âš  æ— æ•ˆå€¼ï¼Œä¿æŒåŸå€¼")
            
            print("\nâœ“ é…ç½®å·²æ›´æ–°")
            continue
        
        if not prompt:
            print("âš  æç¤ºè¯ä¸èƒ½ä¸ºç©º")
            continue
        
        # ç”Ÿæˆ
        try:
            generate_text(model, tokenizer, prompt, **config)
        except Exception as e:
            print(f"âš  ç”Ÿæˆå¤±è´¥: {e}")


def batch_generate(model, tokenizer, prompts_file: str, output_file: str):
    """æ‰¹é‡ç”Ÿæˆæ¨¡å¼"""
    print(f"\næ‰¹é‡ç”Ÿæˆæ¨¡å¼")
    print(f"è¾“å…¥æ–‡ä»¶: {prompts_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # è¯»å–æç¤ºè¯
    with open(prompts_file, 'r', encoding='utf-8') as f:
        prompts = [line.strip() for line in f if line.strip()]
    
    print(f"å…± {len(prompts)} ä¸ªæç¤ºè¯")
    
    # ç”Ÿæˆ
    all_results = []
    for i, prompt in enumerate(prompts, 1):
        print(f"\n[{i}/{len(prompts)}] ç”Ÿæˆä¸­...")
        texts = generate_text(model, tokenizer, prompt, num_samples=1)
        all_results.append({
            'prompt': prompt,
            'generated': texts[0]
        })
    
    # ä¿å­˜ç»“æœ
    import json
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def demo_mode(model, tokenizer):
    """æ¼”ç¤ºæ¨¡å¼ - ä½¿ç”¨é¢„è®¾æç¤ºè¯"""
    print("\n" + "="*60)
    print("ğŸ­ æ­¦ä¾ é£æ ¼æ–‡æœ¬ç”Ÿæˆ - æ¼”ç¤ºæ¨¡å¼")
    print("="*60)
    
    demo_prompts = [
        "å‰‘å…‰ä¸€é—ª",
        "å°‘å¹´ç¼“ç¼“æŠ¬èµ·å¤´",
        "æ±Ÿæ¹–ä¹‹ä¸­",
        "ä»–çªç„¶è½¬èº«",
        "è¡€æŸ“é•¿ç©º",
        "æœˆé»‘é£é«˜ä¹‹å¤œ",
        "ä¸€å£°é•¿å•¸",
        "åˆ€å‰‘ç›¸äº¤",
    ]
    
    print(f"\nå°†ä½¿ç”¨ {len(demo_prompts)} ä¸ªé¢„è®¾æç¤ºè¯è¿›è¡Œæ¼”ç¤ºï¼š")
    for i, p in enumerate(demo_prompts, 1):
        print(f"  {i}. {p}")
    
    input("\næŒ‰Enterå¼€å§‹...")
    
    for prompt in demo_prompts:
        generate_text(
            model, tokenizer, prompt,
            max_length=150,
            temperature=0.9,
            num_samples=1
        )
        input("\næŒ‰Enterç»§ç»­ä¸‹ä¸€ä¸ª...")


def main():
    parser = argparse.ArgumentParser(description="æ­¦ä¾ é£æ ¼æ–‡æœ¬ç”Ÿæˆ")
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--mode", type=str, default="interactive",
                       choices=["interactive", "batch", "demo"],
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--prompts_file", type=str, help="æ‰¹é‡æ¨¡å¼ï¼šæç¤ºè¯æ–‡ä»¶")
    parser.add_argument("--output_file", type=str, help="æ‰¹é‡æ¨¡å¼ï¼šè¾“å‡ºæ–‡ä»¶")
    
    args = parser.parse_args()
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(args.model_path)
    
    # æ ¹æ®æ¨¡å¼è¿è¡Œ
    if args.mode == "interactive":
        interactive_mode(model, tokenizer)
    elif args.mode == "demo":
        demo_mode(model, tokenizer)
    elif args.mode == "batch":
        if not args.prompts_file or not args.output_file:
            print("âš  æ‰¹é‡æ¨¡å¼éœ€è¦æŒ‡å®š --prompts_file å’Œ --output_file")
            return
        batch_generate(model, tokenizer, args.prompts_file, args.output_file)


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    print("="*60)
    print("æ­¦ä¾ é£æ ¼æ–‡æœ¬ç”Ÿæˆ - æ¨ç†è„šæœ¬")
    print("="*60)
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("\n1. äº¤äº’æ¨¡å¼:")
    print("   python inference.py --model_path ./output/wuxia_model/final_model")
    print("\n2. æ¼”ç¤ºæ¨¡å¼:")
    print("   python inference.py --model_path ./output/wuxia_model/final_model --mode demo")
    print("\n3. æ‰¹é‡ç”Ÿæˆ:")
    print("   python inference.py --model_path ./output/wuxia_model/final_model --mode batch \\")
    print("       --prompts_file prompts.txt --output_file results.json")
    print("="*60)
    print()
    
    main()
